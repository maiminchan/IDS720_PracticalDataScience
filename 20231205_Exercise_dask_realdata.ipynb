{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing ARCOS Data with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we used dask to play with a few datasets to get a feel for how dask works. In order to help us develop code that would run quickly, however, we worked with very small, safe datasets. \n",
    "\n",
    "Today, we will continue to work with dask, but this time using much larger datasets. This means that (a) doing things incorrectly may lead to your computer crashing (So save all your open files before you start!), and (b) many of the commands you are being asked run will take several minutes each. \n",
    "\n",
    "For familiarity, and so you can see what advantages dask can bring to your workflow, today we'll be working with the DEA ARCOS drug shipment database published by the Washington Post! However, to strike a balance between size and speed, we'll be working with a slightly thinned version that has only the last two years of data, instead of all six."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Download the thinned ARCOS data [from this link](https://www.dropbox.com/s/o7nc6yvrwog4ozi/arcos_2011_2012.tsv.zip?dl=0). It should be about 2GB zipped, 25 GB unzipped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask\n",
      "  Downloading dask-2023.12.0-py3-none-any.whl (1.2 MB)\n",
      "                                              0.0/1.2 MB ? eta -:--:--\n",
      "                                              0.0/1.2 MB ? eta -:--:--\n",
      "                                              0.0/1.2 MB ? eta -:--:--\n",
      "                                              0.0/1.2 MB ? eta -:--:--\n",
      "                                              0.0/1.2 MB ? eta -:--:--\n",
      "     -                                        0.0/1.2 MB 217.9 kB/s eta 0:00:06\n",
      "     --                                       0.1/1.2 MB 297.7 kB/s eta 0:00:04\n",
      "     ------------                             0.4/1.2 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------              0.8/1.2 MB 3.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.2/1.2 MB 3.6 MB/s eta 0:00:00\n",
      "Collecting click>=8.1 (from dask)\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "                                              0.0/97.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 97.9/97.9 kB 5.8 MB/s eta 0:00:00\n",
      "Collecting cloudpickle>=1.5.0 (from dask)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Collecting fsspec>=2021.09.0 (from dask)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "                                              0.0/166.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 166.4/166.4 kB 9.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from dask) (23.1)\n",
      "Collecting partd>=1.2.0 (from dask)\n",
      "  Downloading partd-1.4.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from dask) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from dask) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from dask) (6.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from click>=8.1->dask) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\lenovo\\miniconda3\\lib\\site-packages (from importlib-metadata>=4.13.0->dask) (3.16.2)\n",
      "Collecting locket (from partd>=1.2.0->dask)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: locket, fsspec, cloudpickle, click, partd, dask\n",
      "Successfully installed click-8.1.7 cloudpickle-3.0.0 dask-2023.12.0 fsspec-2023.10.0 locket-1.0.0 partd-1.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "file_path = r\"D:\\Duke\\Fall 23\\Data Python\\arcos_2011_2012.tsv\"\n",
    "\n",
    "# Use dask to read the data\n",
    "df = dd.read_csv(file_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                 int64\n",
      "REPORTER_DEA_NO           object\n",
      "REPORTER_BUS_ACT          object\n",
      "REPORTER_NAME             object\n",
      "REPORTER_ADDL_CO_INFO     object\n",
      "REPORTER_ADDRESS1         object\n",
      "REPORTER_ADDRESS2        float64\n",
      "REPORTER_CITY             object\n",
      "REPORTER_STATE            object\n",
      "REPORTER_ZIP               int64\n",
      "REPORTER_COUNTY           object\n",
      "BUYER_DEA_NO              object\n",
      "BUYER_BUS_ACT             object\n",
      "BUYER_NAME                object\n",
      "BUYER_ADDL_CO_INFO        object\n",
      "BUYER_ADDRESS1            object\n",
      "BUYER_ADDRESS2            object\n",
      "BUYER_CITY                object\n",
      "BUYER_STATE               object\n",
      "BUYER_ZIP                  int64\n",
      "BUYER_COUNTY              object\n",
      "TRANSACTION_CODE          object\n",
      "DRUG_CODE                  int64\n",
      "NDC_NO                     int64\n",
      "DRUG_NAME                 object\n",
      "QUANTITY                 float64\n",
      "UNIT                     float64\n",
      "ACTION_INDICATOR         float64\n",
      "ORDER_FORM_NO            float64\n",
      "CORRECTION_NO            float64\n",
      "STRENGTH                 float64\n",
      "TRANSACTION_DATE           int64\n",
      "CALC_BASE_WT_IN_GM       float64\n",
      "DOSAGE_UNIT              float64\n",
      "TRANSACTION_ID             int64\n",
      "Product_Name              object\n",
      "Ingredient_Name           object\n",
      "Measure                   object\n",
      "MME_Conversion_Factor    float64\n",
      "Combined_Labeler_Name     object\n",
      "Revised_Company_Name      object\n",
      "Reporter_family           object\n",
      "dos_str                  float64\n",
      "date                      object\n",
      "year                       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read a small sample of the data\n",
    "sample_df = pd.read_csv(file_path, sep=\"\\t\", nrows=100)\n",
    "\n",
    "# Inspect the inferred data types\n",
    "print(sample_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a small sample of the data using Pandas to infer data types\n",
    "sample_df = pd.read_csv(file_path, sep=\"\\t\", nrows=100)\n",
    "dtype_dict = sample_df.dtypes.to_dict()\n",
    "\n",
    "# Use Dask to read the entire dataset with inferred data types\n",
    "df = dd.read_csv(file_path, sep=\"\\t\", dtype=dtype_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Our goal today is going to be to find the pharmaceutical company that has shipped the most opioids (`MME_Conversion_Factor * CALC_BASE_WT_IN_GM`) in the US.\n",
    "\n",
    "When working with large datasets, it is good practice to begin by prototyping your code with a subset of your data. So begin by using `pandas` to read in the first 100,000 lines of the ARCOS data and write pandas code to compute the shipments from each shipper (the group that reported the shipment). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 REPORTER_DEA_NO REPORTER_BUS_ACT               REPORTER_NAME  \\\n",
      "0           0       PA0006836      DISTRIBUTOR  ACE SURGICAL SUPPLY CO INC   \n",
      "1           9       PA0021179      DISTRIBUTOR                APOTHECA INC   \n",
      "2          10       PA0021179      DISTRIBUTOR                APOTHECA INC   \n",
      "3          16       PA0021179      DISTRIBUTOR                APOTHECA INC   \n",
      "4          17       PA0021179      DISTRIBUTOR                APOTHECA INC   \n",
      "\n",
      "  REPORTER_ADDL_CO_INFO  REPORTER_ADDRESS1 REPORTER_ADDRESS2 REPORTER_CITY  \\\n",
      "0                   NaN  1034 PEARL STREET               NaN      BROCKTON   \n",
      "1                   NaN     1622 N 16TH ST               NaN       PHOENIX   \n",
      "2                   NaN     1622 N 16TH ST               NaN       PHOENIX   \n",
      "3                   NaN     1622 N 16TH ST               NaN       PHOENIX   \n",
      "4                   NaN     1622 N 16TH ST               NaN       PHOENIX   \n",
      "\n",
      "  REPORTER_STATE  REPORTER_ZIP  ...                          Product_Name  \\\n",
      "0             MA          2301  ...  HYDROCODONE BIT/ACETA 10MG/500MG USP   \n",
      "1             AZ         85006  ...  HYDROCODONE BITARTRATE & ACETA  5MG/   \n",
      "2             AZ         85006  ...  HYDROCODONE BITARTRATE & ACETA  5MG/   \n",
      "3             AZ         85006  ...  HYDROCODONEBITARTRATE & ACETA  7.5MG   \n",
      "4             AZ         85006  ...  HYDROCODONE BITARTRATE & ACETA  5MG/   \n",
      "\n",
      "                           Ingredient_Name Measure MME_Conversion_Factor  \\\n",
      "0  HYDROCODONE BITARTRATE HEMIPENTAHYDRATE     TAB                   1.0   \n",
      "1  HYDROCODONE BITARTRATE HEMIPENTAHYDRATE     TAB                   1.0   \n",
      "2  HYDROCODONE BITARTRATE HEMIPENTAHYDRATE     TAB                   1.0   \n",
      "3  HYDROCODONE BITARTRATE HEMIPENTAHYDRATE     TAB                   1.0   \n",
      "4  HYDROCODONE BITARTRATE HEMIPENTAHYDRATE     TAB                   1.0   \n",
      "\n",
      "  Combined_Labeler_Name Revised_Company_Name             Reporter_family  \\\n",
      "0            SpecGx LLC         Mallinckrodt  ACE Surgical Supply Co Inc   \n",
      "1         Apotheca Inc.        Apotheca Inc.                Apotheca Inc   \n",
      "2         Apotheca Inc.        Apotheca Inc.                Apotheca Inc   \n",
      "3         Apotheca Inc.        Apotheca Inc.                Apotheca Inc   \n",
      "4         Apotheca Inc.        Apotheca Inc.                Apotheca Inc   \n",
      "\n",
      "  dos_str        date  year  \n",
      "0    10.0  2012-12-26  2012  \n",
      "1     5.0  2012-12-05  2012  \n",
      "2     5.0  2012-07-24  2012  \n",
      "3     7.5  2012-02-04  2012  \n",
      "4     5.0  2011-11-07  2011  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_18040\\3296500675.py:1: DtypeWarning: Columns (4,6,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  arcos_subset = pd.read_csv(file_path, sep=\"\\t\", nrows=100000)\n"
     ]
    }
   ],
   "source": [
    "arcos_subset = pd.read_csv(file_path, sep=\"\\t\", nrows=100000)\n",
    "\n",
    "# Check the structure of the data and inspect the first few rows\n",
    "print(arcos_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shipper with the most shipments:\n",
      "REPORTER_BUS_ACT                DISTRIBUTOR\n",
      "MME_Conversion_Factor              126572.5\n",
      "CALC_BASE_WT_IN_GM            342829.438631\n",
      "Total_Shipments          43392779121.144394\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Compute shipments from each shipper\n",
    "shipments_by_shipper = (\n",
    "    arcos_subset.groupby(\"REPORTER_BUS_ACT\")\n",
    "    .agg({\"MME_Conversion_Factor\": \"sum\", \"CALC_BASE_WT_IN_GM\": \"sum\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Create a new column for total shipments (MME * CALC_BASE_WT_IN_GM)\n",
    "shipments_by_shipper[\"Total_Shipments\"] = (\n",
    "    shipments_by_shipper[\"MME_Conversion_Factor\"]\n",
    "    * shipments_by_shipper[\"CALC_BASE_WT_IN_GM\"]\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by total shipments in descending order\n",
    "shipments_by_shipper = shipments_by_shipper.sort_values(\n",
    "    by=\"Total_Shipments\", ascending=False\n",
    ")\n",
    "\n",
    "# Display the result (shipper with the most shipments)\n",
    "print(\"Shipper with the most shipments:\")\n",
    "print(shipments_by_shipper.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Now let's turn to dask. Re-write your code for dask, and calculate the total shipments by reporting company. Remember: \n",
    "\n",
    "- Activate a conda environment with a clean dask installation.\n",
    "- Start by spinning up a distributed cluster.\n",
    "- Dask won't read compressed files, so you have to unzip your ARCOS data. \n",
    "- Start your cluster in a cell all by itself since you don't want to keep re-running the \"start a cluster\" code. \n",
    "\n",
    "If you need to review dask basic code, [check here](https://nickeubank.github.io/practicaldatascience_book/notebooks/PDS_not_yet_in_coursera/30_big_data/70_dask.html).\n",
    "\n",
    "As you run your code, make sure to click on the Dashboard link below where you created your cluster:\n",
    "\n",
    "![dask_dashboard](images/dask_cluster.png)\n",
    "\n",
    "Among other things, the bar across the bottom should give you a sense of how long your task will take:\n",
    "\n",
    "![dask_progress](images/dask_progress.png)\n",
    "\n",
    "(For context, my computer (which has 10 cores) only took a couple seconds. My computer is fast, but most computers should be done within a couple minutes, tops).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Now let's calculate, *for each state*, what company shipped the most pills?\n",
    "\n",
    "Note you will quickly find that you can't sort in dask -- sorting in parallel is *really* tricky! So you'll have to work around that. Do what you need to do on the big dataset first, then compute it all so you get it as a regular pandas dataframe, then finish. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this seem like a situation where a single company is responsible for the opioid epidemic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 \n",
    "\n",
    "Now go ahead and try and re-do the chunking you did by hand for your project (with this 2 years of data) -- calculate, for each year, the total morphine equivalents sent to each county in the US. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "Now, re-write your opioid project's initial opioid import using dask. Each person on your team should create a NEW branch to try this. The person who wrote the initial chunking code can help everyone else understand what they did originally and the data, but everyone should write their own code. \n",
    "\n",
    "**WARNING:** You will probably run into a lot of type errors (depending on how the ARCOS data has changed since last year). With real world messy data one of the biggest problems with dask is that it struggles if halfway through dataset it discovers that the column it *thought* was floats contains text. That's why, in the dask reading, [I specified the column type for so many columns](https://nickeubank.github.io/practicaldatascience_book/notebooks/PDS_not_yet_in_coursera/30_big_data/70_dask.html#what-can-dask-do-for-me) as `objects` explicitly. Then, because occasionally there data cleanliness issues, I had to do some converting data types by hand. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
